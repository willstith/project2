{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping: BeautifulSoup\n",
    "_Collecting data from the internet and parsing it into meaningful (often tabular) form._\n",
    "\n",
    "### Docs\n",
    "\n",
    "- [BeautifulSoup documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "\n",
    "### Installation\n",
    "\n",
    "If not using the Metis kernel, please install the following libraries\n",
    "\n",
    "With conda:\n",
    "- `conda install beautifulsoup4 requests lxml html5lib`\n",
    "\n",
    "With pip:\n",
    "- `pip install beautifulsoup4 requests lxml html5lib`\n",
    "\n",
    "If you have installed everything correctly, you should now be able to import these libraries. (You may need to shutdown and restart this notebook for BeautifulSoup to recognize the `lxml` and `html5lib` parsers.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is BeautifulSoup?\n",
    "\n",
    "- Python library\n",
    "- **HTML parser**:  Interprets structure of HTML file \n",
    "- Does not actually get pages from the web.  Use `requests` library for that.\n",
    "\n",
    "<br>\n",
    "<img src=\"images/web_scraping_pipeline.png\" alt=\"Web Scraping Pipeline\" style=\"width: 650px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to HTML\n",
    "\n",
    "_Basic language used to create a webpage._\n",
    "\n",
    "- Tells browser what, where, and how to display text, images, and other media\n",
    "- Structured, hierarchical nature \n",
    "- Comprised of \"elements\" with properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example HTML element\n",
    "```html\n",
    "<tag-name attr1=\"value of attr1\" attr2=\"value of attr2\" .... attrN=\"value of attrN\">\n",
    "    Inner text of the tag\n",
    "</tag-name>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tags,  `<tag-name>`\n",
    "\n",
    "- Elements are labeled with tags\n",
    "- Tells us what type of \"thing\" to render\n",
    "\n",
    "    \n",
    "| Tag | Use\n",
    "|---          | ---|\n",
    "|`h1`, `h2`, ..., `h6`| headers|\n",
    "|`p`| paragraphs |\n",
    "|`a`| anchors (e.g. links) |\n",
    "|`div`| divisions (or sections) of a page |\n",
    "|`img`| images |\n",
    "|`li` | list items |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes, `attr1`\n",
    "\n",
    "- Special properties we want this tag to have\n",
    "- Typically appear as `attribute name = \"attribute value\"` pair\n",
    "\n",
    "| Attribute | Use | Notes\n",
    "|---   | ---       | ---|\n",
    "|`href` | hyperlink reference | Clicking this element directs user to value url|\n",
    "|`class`| style class | Many elements may have same class |\n",
    "|`id`| unique identifier | Only one element per id! |\n",
    "|`style`| extra element styling | Bad practice, use css instead |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner HTML Text\n",
    "\n",
    "- Text that appears between tags\n",
    "- Often the information we want to extract during web scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML Structure\n",
    "\n",
    "A full HTML document has a structure similar to this:\n",
    "\n",
    "```html\n",
    "<html> \n",
    "  <head> </head>\n",
    "  <body>\n",
    "     <h1>This is a header</h1>\n",
    "     <p style=\"color:red;\" id=\"learning_paragraph\">You are learning HTML</p>\n",
    "     <a href=\"www.google.com\">A link to Google</a>\n",
    "  </body>\n",
    "</html>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTIONS**\n",
    "> How many elements do we have within the HTML body?  What are their tags?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What is the inner HTML of the header element?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What attribute(s) does the paragraph have?  And the attribute value(s)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving this code as a .html file and opening it in a browser should yield:\n",
    "\n",
    "<br>\n",
    "<img src=\"images/example_html.png\" alt=\"Rendering of Example HTML\" style=\"width: 300px;\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn to Scrape with Dummy HTML\n",
    "\n",
    "Let's begin learning how to scrape by working with some dummy HTML, written below as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_html = \"\"\"\n",
    "<html>\n",
    "\n",
    "<head>\n",
    "</head>\n",
    "\n",
    "<body>\n",
    "    <div style=\"border: 1px solid\">\n",
    "        There isn't much in this file, except a list of to-do items. \n",
    "        <ul>\n",
    "          <li>Make coffee</li>\n",
    "          <li>Sweep the floor</li>\n",
    "          <li>Go to the store</li>\n",
    "          <li>Write BeautifulSoup lecture</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "</body>\n",
    "\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at this simple webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<html>\n",
       "\n",
       "<head>\n",
       "</head>\n",
       "\n",
       "<body>\n",
       "    <div style=\"border: 1px solid\">\n",
       "        There isn't much in this file, except a list of to-do items. \n",
       "        <ul>\n",
       "          <li>Make coffee</li>\n",
       "          <li>Sweep the floor</li>\n",
       "          <li>Go to the store</li>\n",
       "          <li>Write BeautifulSoup lecture</li>\n",
       "        </ul>\n",
       "    </div>\n",
       "</body>\n",
       "\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(my_html))     # make sure Jupyter knows to display it as HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to grab the four items on our to-do list and analyze them, we can use Beautiful Soup!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(my_html, \"html5lib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply looking at `soup` isn't very useful -- it's just our HTML repeated back to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html><head>\n",
       "</head>\n",
       "\n",
       "<body>\n",
       "    <div style=\"border: 1px solid\">\n",
       "        There isn't much in this file, except a list of to-do items. \n",
       "        <ul>\n",
       "          <li>Make coffee</li>\n",
       "          <li>Sweep the floor</li>\n",
       "          <li>Go to the store</li>\n",
       "          <li>Write BeautifulSoup lecture</li>\n",
       "        </ul>\n",
       "    </div>\n",
       "\n",
       "\n",
       "\n",
       "</body></html>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.find()`\n",
    "\n",
    "But Beautiful Soup also knows how to navigate this HTML.  We can use the `find` command to get to a specific element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<li>Make coffee</li>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('li')  #Grabs the first element tagged as li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.Tag"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(soup.find('li'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`find` returns a tagged element, but we can go further and just select this element's inner HTML text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Make coffee'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('li').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(soup.find('li').text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.find_all()`\n",
    "\n",
    "Instead of selecting just one of our list items, we can get all of them by using `find_all`.  \n",
    "\n",
    "This method looks for all instances matching our criteria on the entire HTML and gives us back a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<li>Make coffee</li>,\n",
       " <li>Sweep the floor</li>,\n",
       " <li>Go to the store</li>,\n",
       " <li>Write BeautifulSoup lecture</li>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('li')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze our to-do list, we probably just want the text from each tagged element.  How can we do that?\n",
    "\n",
    "One approach is to loop through the list and apply `.text` to each element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Make coffee', 'Sweep the floor', 'Go to the store', 'Write BeautifulSoup lecture']\n"
     ]
    }
   ],
   "source": [
    "todos=[]\n",
    "\n",
    "for element in soup.find_all('li'):\n",
    "    todos.append(element.text)\n",
    "    \n",
    "print(todos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we could use a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Make coffee',\n",
       " 'Sweep the floor',\n",
       " 'Go to the store',\n",
       " 'Write BeautifulSoup lecture']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "todos=[element.text for element in soup.find_all('li')]\n",
    "\n",
    "todos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a clean list of strings, ready for analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Select Items on a Test Webpage\n",
    "\n",
    "Now on to a more complicated example.  Take a look at `test_webpage/page.html`.  Let's try to grab all of the article links, like the ones for Starbucks and Bitcoin.\n",
    "\n",
    "First get the HTML and then parse it with BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test_webpage/page.html'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-60d7433e3262>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_webpage/page.html'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtest_html\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_html\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lxml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test_webpage/page.html'"
     ]
    }
   ],
   "source": [
    "with open('test_webpage/page.html') as page:\n",
    "    test_html = page.read()\n",
    "soup = BeautifulSoup(test_html, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links show up as `a` tags.  Let's just try to grab all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh oh!  Looks like there are some links on the sidebar and in the footer, too.  We want only the ones in the articles so we'll need a better strategy.\n",
    "\n",
    "Digging into the source code, it turns out that each of the articles live within a `div` labeled with the class `article`.  Let's try to get those.\n",
    "\n",
    "### `class_` and `id_` \n",
    "The `find` and `find_all` methods take optional attribute arguments so you can filter down to elements with specific attributes like classes and ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('div', class_='article')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are our articles!  \n",
    "\n",
    "Each of these `div` elements are also soup objects, so we can now query these `div`s to drill down further to just the links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for div in soup.find_all('div', class_='article'):\n",
    "    for link in div.find_all('a'):\n",
    "        print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent!  What if we want to print out the link text and the url it points to?\n",
    "\n",
    "\n",
    "### `.get()`\n",
    "The `get` method allows you access to any attribute of the element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for div in soup.find_all('div', class_='article'):\n",
    "    for link in div.find_all('a'):\n",
    "        print(f'{link.text:<20} ---> {link.get(\"href\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape the Web\n",
    "\n",
    "So far we've used BeautifulSoup to parse our own HTML strings and files.  Now let's scrape Box Office Mojo.\n",
    "\n",
    "First let's take a look at some source code for _The Big Lebowski_.\n",
    "\n",
    "- Navigate to https://www.boxofficemojo.com/title/tt0118715/ in your browser, preferably Chrome\n",
    "- Right click and select \"Inspect\"\n",
    "- Alternatively, you can \"View Page Source\"\n",
    "\n",
    "To retrieve the HTML for this webpage, we will use `requests`.\n",
    "\n",
    "### `requests`\n",
    "\n",
    "The `requests` library allows us to grab information from the web.  There are two common types of requests:\n",
    "- `get` -- simply requests information, akin to putting a url in your browser\n",
    "- `post` -- sends information to the website, for example, writing an email\n",
    "\n",
    "We will be using `get` to retrieve a page's HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.boxofficemojo.com/title/tt0118715/' \n",
    "\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response we got back is an object that gives us access to:\n",
    "- `response.text` -- the returned HTML (if any)\n",
    "- `response.json` -- the returned JSON (if any), typical for APIs\n",
    "- `response.status_code` -- a [code](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes) to tell you if your request was successful or if an error occurred, 2XX indicates success while 404 means not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.status_code  #200 = success!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.text[:1000]  #First 1000 characters of the HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `BeautifulSoup` Basics\n",
    "\n",
    "Now that we have the HTML, let's learn its structure by parsing with BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `prettify` method turns the soup into a nicely formatted Unicode string with one tag on each line for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTIONS**\n",
    "\n",
    "> Select the first link on the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now select the LAST link on the page.  Can you get the text and the URL associated with this link?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember `find` gets only one match, but `find_all` retrieves all matches in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in soup.find_all('a')[:5]:\n",
    "    print(link, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can match only those with a specific `id` or `class` if you'd like.  Here are all the elements labeled with the \"mojo-navigation-tab\" class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in soup.find_all(class_='mojo-navigation-tab'):\n",
    "    print(element.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to remember `find` and `find_all` return BeautifulSoup elements. You can continue searching these elements, thus chaining commands together.\n",
    "\n",
    "Basic earnings information can be found in the `div` with the \"mojo-performance-summary-table\" class.  Let's extract the domestic gross from this element.\n",
    "\n",
    "<br>\n",
    "<img src=\"images/biglebow_table.png\" alt=\"Big Lebowski Table\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.find(class_='mojo-performance-summary-table').prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(class_='mojo-performance-summary-table').find_all('span', class_='money')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text needs to be extracted from one element at a time.  To get the domestic gross:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(class_='mojo-performance-summary-table').find_all('span', class_='money')[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also find using an `id`; remember id should be unique to just one element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(soup.find(id='tabs').prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping Pipeline\n",
    "\n",
    "Now that we have the basics, let's practice web scraping.  **The main goal of web scraping is to extract data by taking advantage of a site's consistent format.**  That is, the code you write for one page on a website can hopefully be used on multiple pages to gather more information automatically.\n",
    "\n",
    "Let's create code to get the following information for the movies on Box Office Mojo:\n",
    "- Movie title\n",
    "- Domestic gross\n",
    "- Runtime\n",
    "- MPAA rating\n",
    "- Release date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_string = soup.find('title').text\n",
    "\n",
    "title_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_string.split('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = title_string.split('-')[0].strip()\n",
    "\n",
    "title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Domestic Gross: \n",
    "\n",
    "As we saw previously, the domestic gross can be found in a `span` within the \"mojo-performance-summary-table\" `div`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtg = soup.find(class_='mojo-performance-summary-table').find_all('span', class_='money')[0].text\n",
    "dtg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remainder of the information lives in this neighboring `div`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/biglebow_info.png\" alt=\"Big Lebowski Information\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runtime: `.findNext()`\n",
    "\n",
    "Sometimes you can find the information you are looking for by using text matching.  But note this must be an exact match!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(text='Run')  #does not match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(text='Running Time')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could use [regular expressions](https://docs.python.org/3/library/re.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "runtime_regex = re.compile('Run')\n",
    "soup.find(text=runtime_regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_string = soup.find(text=re.compile('Run'))\n",
    "print(rt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(rt_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The string we found is still a Beautiful Soup element. This means we can use it to navigate to the next element in the HTML, which is a `span` containing the actual runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_string.findNext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.findNext()` method can be incredibly useful when the information you want to find doesn't have a obvious tag, class, id, etc.\n",
    "\n",
    "Let's clean this value up into usable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt = rt_string.findNext().text\n",
    "rt = rt.split()\n",
    "minutes = int(rt[0])*60 + int(rt[2])\n",
    "print(minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MPAA Rating, Release Date\n",
    "\n",
    "_**STEP 1:** Create function to grab values_ \n",
    "\n",
    "The text matching method can also help us get runtime, rating, and release date, so let's make a reuable function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_value(soup, field_name):\n",
    "    \n",
    "    '''Grab a value from Box Office Mojo HTML\n",
    "    \n",
    "    Takes a string attribute of a movie on the page and returns the string in\n",
    "    the next sibling object (the value for that attribute) or None if nothing is found.\n",
    "    '''\n",
    "    \n",
    "    obj = soup.find(text=re.compile(field_name))\n",
    "    \n",
    "    if not obj: \n",
    "        return None\n",
    "    \n",
    "    # this works for most of the values\n",
    "    next_element = obj.findNext()\n",
    "    \n",
    "    if next_element:\n",
    "        return next_element.text \n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runtime\n",
    "runtime = get_movie_value(soup,'Run')\n",
    "print(runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rating\n",
    "rating = get_movie_value(soup,'MPAA')\n",
    "print(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_date = get_movie_value(soup,'Release Date')\n",
    "print(release_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_date = release_date.split('\\n')[0]  #Select the only the date\n",
    "print(release_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**STEP 2:** Create helper functions to parse strings into appropriate data types_\n",
    "\n",
    "The returned values all need a bit of formatting before we can work with this data.  Here are a few helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateutil.parser\n",
    "\n",
    "def money_to_int(moneystring):\n",
    "    moneystring = moneystring.replace('$', '').replace(',', '')\n",
    "    return int(moneystring)\n",
    "\n",
    "def runtime_to_minutes(runtimestring):\n",
    "    runtime = runtimestring.split()\n",
    "    try:\n",
    "        minutes = int(runtime[0])*60 + int(runtime[2])\n",
    "        return minutes\n",
    "    except (IndexError, ValueError):\n",
    "        return None\n",
    "\n",
    "def to_date(datestring):\n",
    "    date = dateutil.parser.parse(datestring)\n",
    "    return date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**STEP 3:** Apply these conversions_\n",
    "\n",
    "Let's get these values again and format them all in one swoop. (Note: Rating is already correct as a string.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_domestic_total_gross = dtg\n",
    "domestic_total_gross = money_to_int(raw_domestic_total_gross)\n",
    "\n",
    "raw_runtime = get_movie_value(soup,'Running')\n",
    "runtime = runtime_to_minutes(raw_runtime)\n",
    "\n",
    "raw_release_date = get_movie_value(soup,'Release Date').split('\\n')[0]\n",
    "release_date = to_date(raw_release_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Put Results in Dictionary\n",
    "\n",
    "Now that we have results for all five quantities, we can store them in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['movie title', 'domestic total gross',\n",
    "           'runtime (mins)', 'rating', 'release date']\n",
    "\n",
    "movie_data = []\n",
    "movie_dict = dict(zip(headers, [title,\n",
    "                                domestic_total_gross,\n",
    "                                runtime,\n",
    "                                rating, \n",
    "                                release_date]))\n",
    "\n",
    "movie_data.append(movie_dict)\n",
    "movie_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION**\n",
    "\n",
    "> Why might we want to store these data in a dictionary?  Why did we put the dictionary in a list?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Tables\n",
    "\n",
    "Let's take a look at the [top G-rated movies](https://www.boxofficemojo.com/chart/mpaa_title_lifetime_gross/?by_mpaa=G) of Box Office Mojo.  How could we pull all the data from this main page?\n",
    "\n",
    "First request the HTML and parse it with Beautiful Soup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.boxofficemojo.com/chart/mpaa_title_lifetime_gross/?by_mpaa=G'\n",
    "\n",
    "response = requests.get(url)\n",
    "page = response.text\n",
    "\n",
    "soup = BeautifulSoup(page,\"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now find the main table; its the only `table` on the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.find('table')\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [row for row in table.find_all('tr')]  # tr tag is for rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row contains the information we want but requires more parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember: you can chain methods together to look for information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows[1].find_all('td')[0].find('a')['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now grab data for the first 5 movies with a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = {}\n",
    "\n",
    "for row in rows[1:6]:\n",
    "    items = row.find_all('td')\n",
    "    link = items[0].find('a')\n",
    "    title, url = link.text, link['href']\n",
    "    movies[title] = [url] + [i.text for i in items]\n",
    "    \n",
    "movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Multiple Pages\n",
    "\n",
    "Now that we have the links for several G-rated movies we can visit each link to extract even more information about each movie.  Let's use `pandas` to help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_movies = pd.DataFrame(movies).T  #transpose\n",
    "g_movies.columns = ['link_stub', 'title', 'rank_g_movies', \n",
    "                    'lifetime_gross', 'rank_overall', 'year']\n",
    "\n",
    "g_movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also combine all previous steps into one helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_dict(link):\n",
    "    '''\n",
    "    From BoxOfficeMojo link stub, request movie html, parse with BeautifulSoup, and\n",
    "    collect \n",
    "        - title \n",
    "        - domestic gross\n",
    "        - runtime \n",
    "        - MPAA rating\n",
    "        - full release date\n",
    "    Return information as a dictionary.\n",
    "    '''\n",
    "    \n",
    "    base_url = 'https://www.boxofficemojo.com'\n",
    "    \n",
    "    #Create full url to scrape\n",
    "    url = base_url + link\n",
    "    \n",
    "    #Request HTML and parse\n",
    "    response = requests.get(url)\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page,\"lxml\")\n",
    "\n",
    "    \n",
    "    headers = ['movie_title', 'domestic_total_gross',\n",
    "               'runtime_minutes', 'rating', 'release_date']\n",
    "    \n",
    "    #Get title\n",
    "    title_string = soup.find('title').text\n",
    "    title = title_string.split('-')[0].strip()\n",
    "\n",
    "    #Get domestic gross\n",
    "    raw_domestic_total_gross = (soup.find(class_='mojo-performance-summary-table')\n",
    "                                    .find_all('span', class_='money')[0]\n",
    "                                    .text\n",
    "                               )\n",
    "    domestic_total_gross = money_to_int(raw_domestic_total_gross)\n",
    "\n",
    "    #Get runtime\n",
    "    raw_runtime = get_movie_value(soup,'Running')\n",
    "    runtime = runtime_to_minutes(raw_runtime)\n",
    "    \n",
    "    #Get rating\n",
    "    rating = get_movie_value(soup,'MPAA')\n",
    "\n",
    "    #Get release date\n",
    "    raw_release_date = get_movie_value(soup,'Release Date').split('\\n')[0]\n",
    "    release_date = to_date(raw_release_date)\n",
    "    \n",
    "    #Create movie dictionary and return\n",
    "    movie_dict = dict(zip(headers, [title,\n",
    "                                domestic_total_gross,\n",
    "                                runtime,\n",
    "                                rating, \n",
    "                                release_date]))\n",
    "\n",
    "    return movie_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need to pass each link stub to this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_movies_page_info_list = []\n",
    "\n",
    "for link in g_movies.link_stub:\n",
    "    g_movies_page_info_list.append(get_movie_dict(link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_movies_page_info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_movies_page_info = pd.DataFrame(g_movies_page_info_list)  #convert list of dict to df\n",
    "g_movies_page_info.set_index('movie_title', inplace=True)\n",
    "\n",
    "g_movies_page_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note: the rating is indeed missing from a few of these pages!  How could you fix that?)\n",
    "\n",
    "We can now match this back up with the movie information collected from the table by merging these dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_movies = g_movies.merge(g_movies_page_info, left_index=True, right_index=True)\n",
    "\n",
    "g_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "- Beautiful Soup is a powerful HTML parser\n",
    "- You can locate one element with `.find()` or all matching elements with `.find_all()`\n",
    "- To select specific elements, you can filter by tags like `class` or `id` \n",
    "- You can also find items using text matching and `.findNext()`, `.findNextSibling()`, `.findChild()`, etc.\n",
    "- Once you know how to scrape one page, you can scale up by systematically visiting other similar pages.\n",
    "\n",
    "### Limitations\n",
    "Beautiful Soup has its limitations though.  For example, we can't use Beautiful Soup if a page:\n",
    "- Requires us to input a password\n",
    "- Reveals information we want only when we interact with it\n",
    "- Generates dynamically (with JavaScript) rather than statically serving HTML\n",
    "\n",
    "For these situations we need a different tool, like **Selenium** -- coming soon!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
